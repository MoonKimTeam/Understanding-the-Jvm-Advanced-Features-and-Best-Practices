# 최적화 사례 분석 및 실전

## 사례 분석

### 대용량 메모리 기기 대상 배포 전략

시스템 하드웨어와 소프트웨어 구성을 고려하여 핫스팟 가상 머신을 서버 모드로 실행했고, 기본 설정인 패러렐 컬렉터가 메모리 관리를 책임졌다. <br>
패러렐 컬렉터는 일시 정지 시간보다는 처리량에 중점을 둔 컬렉터다. 그래서 12GB에 달하는 힙 메모리를 전체 GC하기 위해 최장 14초까지 일시 정지하게 된 것이다. <br>
또한 프로그램 설계상 사용자가 웹 페이지를 요청하면 해당 파일을 디스크에서 메모리로 읽어 들인다. <br>
이때 웹 페이지를 직렬화하는 과정에서 메모리에는 수많은 거대 객체가 쌓여 갔다. <br>
거대 객체 대부분은 곧장 구세대에 만들어졌다. 비록 힙을 12GB나 준비해 두었지만 금세 가득 차서 몇 분마다 10초씩 일시 정지되는 사태로 이어졌다.

자바 가상 머신의 모든 가비지 컬렉터슨 특정한 애플리케이션 타입과 동작 시나리오를 목표로 설계되었다. <br>
현재 대용량 메모리를 갖춘 하드웨어에 단일 자바 애플리케이션을 배포하는 주된 방식은 다음 두 가지다. <br>

1. 가상 머신 인스턴스 하나가 거대한 자바 힙 메모리를 관리한다.
2. 가상 머신 여러 개를 동시에 띄워 논리적인 클러스터를 구성한다.

지금 사례에서는 관리자가 첫 번째 방식을 선택했다. 사용자와 상호 작용이 많고 일시 정지 시간에 민감하고 대용량 메모리를 갖추고 있다. <br>
따라서 셰넌도어나 ZGC처럼 지연 시간 통제를 목표로 하는 가비지 컬렉터를 이용하면 이 문제를 해결할 수 있다. <br>
**추가로 여러분이 단일 가상 머신으로 거대 메모리를 관리할 계획이라면 다음 잠재 문제들도 고려해야 한다.**

### 클러스터 간 동기화로 인한 메모리 오버플로

처음에는 공유 데이터를 데이터베이스로 관리했지만 읽기와 쓰기가 잦고 경합이 치열하여 성능에 미치는 영향이 컸다. <br>
그래서 나중에는 JBossCache로 글로벌 캐시를 구축했다. 글로벌 캐시를 구축한 뒤로는 오랜 기간 서비스가 원활하게 운영되었다. <br>
하지만 시간이 지나자 메모리 오버플로가 가끔씩 발생하기 시작했다. <br>
그래서 -XX:+HeapDumpOnOutOfMemoryError 매개 변수를 추가한 다음 서비스를 한동안 운영해보았다. <br>
이 상태에서 발생한 오버플로의 힙 덤프 스냅숏을 살펴보니 수많은 org.jgroups.protocols.pbcast.NAKACK 객체가 발견되었다.

JBossCache는 클러스터 사이의 데이터 통신에 JGroups라는 개념을 이용한다. <br>
JGroups는 데이터 패킷을 보내고 받는 데 필요한 다양한 필수 특성을 자유롭게 조합할 수 있는 프로토콜 스택을 이용한다. <br>
데이터 전송이 100% 성공하리라는 보장은 없다. 따라서 실패 시 재전송을 위해 그룹 멤버십 서비스(GMS)에 등록된 모든 노드가 데이터를 제대로 수신했는지 확인할 때까지는 메모리에 보관해야 한다. <br>
특정 상황에서 네트워크가 데이터 전송량을 다 처리하지 못하게 되면 재전송된 데이터가 메모리에 계속 쌓이다가 오버플로를 일으키는 것이다.

### 힙 메모리 부족으로 인한 오버플로 오류

브라우저-서버 기반 온라인 시험 시스템이었는데, 서버 푸시 기술을 활용해 클라이언트가 서버로부터 시험 데이터를 실시간으로 받아 볼 수 있었다. <br>
테스트 중 서버에서 메모리 오버플로가 이따금 발생했다. 관리자는 힙 메모리를 최대로 늘려서 해결하고자 시도했지만, 여전히 오버플로가 발생했다. <br>
여유 메모리가 부족해진 탓에 -XX:+HeapDu叩OnOutOfMemoryError 매개 변수를 추가해도 시스템은 덤프 파일을 생성해 내지 못했다. <br>
GC는 자주 일어나, 에덴, 생존자, 구세대, 메서드 영역 모두 안정적이였다.

```
[org.eclipse.jetty.util.log] handle failed java.lang.OutOfMemoryError: null
at sun.misc.Unsafe.allocateMemory(Native Method)
at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:99)
at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288)
at org.eclipse.jetty.io.nio.DirectNIOBuffer.<init>
```

운영 체제에는 개별 프로세스가 관리할 수 있는 메모리 최대 크기에 제한이 있다. <br>
지금 사례에서 사용한 32비트 윈도우의 경우 그 크기가 2GB이고, 이 중 1.6GB를 자바 힙에 할당한 상황이다. <br>
한편 다이렉트 메모리는 힙에 속하지 않으므로 남은 0.4GB에서 일부를 떼어 할당할 수밖에 없다. <br>
다이렉트 메모리 역시 가비지 컬렉션의 대상이다. 하지만 힙과 달리 공간이 부족하더라도 가비지 컬렉터에 능동적으로 알리지 못한다.

### 시스템을 느려지게 하는 외부 명령어

사용자 요청을 처리하려면 특정한 시스템 정보가 필요해서 요청 각각이 외부 셸 스크립트를 실행하도록 작성했다는 것이다. <br>
셸 스크립트는 자바의 Runtime.getRuntime().exec() 메서드롤 실행했다. 이 방식으로는 셸 스크립트를 실행한다는 목적은 이룰 수 있지만 자원을 매우 많이 소비한다. <br>
외부 명령 자체는 빠르게 실행되더라도 빈번히 호출되는 경우라면 프로세스 생 성 비용을 간과해서는 안 된다. <br>
자바 가상 머신에서 이 명령을 실행하는 과정은 다음과 같다.

1. 현재 가상 머신과 똑같은 환경 변수 설정을 공유하는 프로세스를 복사한다.
2. 새로운 프로세스에서 외부 명령을 실행한다.
3. 프로세스를 종료한다.

### 서버 가상 머신 프로세스 비정상 종료

한동안 문제없이 운영되던 시스템에서 클러스터 노드의 가상 머신 프로세스가 갑자기 닫히는 일이 빈번해졌다. <br>
시스템 로그를 살펴보니 모든 가상 머신 프로세스가 종료 전에 다음과 같은 예외를 다량으로 쏟아 냈다.

```
java.net.SocketException: Connection reset
at java-net.SocketlnputStream.read(SocketlnputStream.java:168)
at java・ io.BufferedlnputStream・fill(BufferedlnputStream.java:218)
at java.io.BufferedlnputStream.read(BufferedlnputStream.java:235)
at org.apache.axis.transport.http.HTTPSender.readHeadersFromSocket(HTTPSender.
java:583)
at org.apache.axis.transport.http.HTTPSender.invoke(HTTPSender,java:143)
... 99 more
```

최근에 사무 자동화 포털과 연동하는 작업을 수행했다. 경영 정보 시스템에서 할 일 항목의 상태가 바뀌면 사무 자동화 포털 시스템이 웹 서비스를 통해 이 정보를 받아 와서 동기화하는 식이었다. <br>
그런데 동기화 요청을 보내자 최대 3분이 되어서야 응답이 왔고, 그마저도 돌아온 결과는 모두 타임아웃에 의한 연결 중단이었다. <br>
사무 자동화 시스템이 제때 응답해 주지 않아서 대기 중인 스레드와 소켓 연결이 점점 많아졌다. 그러다가 결국 가상 머신의 한계를 넘어서서 가상 머신 프로세스가 비정상 종료된 것이었다. <br>
비동기 호출 부분을 생산자/소비자 방식의 메시지 큐로 변경하니 시스템이 정상으로 돌아왔다.

### 부적절한 데이터 구조로 인한 메모리 과소비

마이너 GC 시간은 보통 30밀리초 안쪽이라서 RPC를 이용하는 외부 서비스들에 아무런 문제를 일으키지 않았다. <br>
하지만 데이터를 분석하기 위해 10분 단위로 80MB 크기의 파일을 메모리로 읽어 들여야 했는데 , 이 때 100만 개 이상의 HashMap<Long, Long> 객체를 만들어 냈다. <br>
그러면 마이너 GC가 100만 개가 넘는 객체를 검사하느라 일시 정지가 500밀리초로 늘어났다.

마이너 GC 후 에덴과 생존자 공간은 거의 빈 상태나 다름없다. <br>
하지만 데이터 파일을 분석하는 동안은 총 800MB 용량의 에덴이 빠르게 채워져서 가비지 컬렉션이 일어나게 된다. <br>
하지만 마이너 GC 후에도 신세대의 객체 대부분이 여전히 살아 있다.

파뉴 컬렉터는 복사 알고리즘을 이용한다. 복사 알고리즘은 대부분의 객체가 '죽어야' 효율이 좋다. <br>
생존한 객체가 매우 많다면 그 모두를 생존자 공간으로 복사하는데, 이 과정에서 객체 사이의 참조들까지 정확하게 관리해야 한다. <br>
매우 무거운 작업일 수밖에 없어서 가비지 컬렉션 시간이 급격히 늘어난 것이다.

### 윈도우 가상 메모리로 인한 긴 일시 정지

이번 문제는 심장 박동을 보여 주는 GUI 데스크톱 프로그램에서 발생했다. <br>
심장 박동 데이터는 서드 파티 서비스로부터 15초마다 얻어 왔으며, 서드 파티 서비스가 30초 내로 회신하지 않으면 연결이 끊긴 것으로 간주했다. <br>
제품 출시 후 거짓 양성（false positive） 데이터가 자주 섞여 들어오는 문제가 발견되었다. <br>
로그를 확인해 보니 거짓 양성이 생기는 이유는 프로그램이 약 1분 간격으로 로그 출력 없이 일시 정지 상태가 되기 때문으로 밝혀졌다.

XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDate-Stamps -Xloggc:gclog.log 매개 변수를 추가한 후 로그를 살펴보자 가비지 컬렉션이 원흉임이 드러났다.

```
Total time for which application threads were stopped: 0.0112389 seconds
Total time for which application threads were stopped: 0.0001335 seconds
Total time for which application threads were stopped: 0.0003246 seconds
Total time for which application threads were stopped: 41.4731411 seconds
```

-XX:+PrintReferenceGC 매개 변수를 추가하여 긴 일시 정지 때의 로그 정보를 자세히 들여다보니, 가비지 컬렉션 자체는 그리 길지 않았다. 그 대신 컬렉션 준비 단계에서 실제 시작까지가 시간을 다 잡아먹고 있었다. <br>
프로그램 창을 최소화하면 메모리 사용량이 급격하게 줄어들었으나 가상 메모리에는 변화가 없었다. <br>
창을 최소화하면 작업 메모리가 디스크로 스와프된다고 짐작되는 상황이었다. <br>
이 상태에서 가비지 컬렉션을 하려면 스와프된 데이터를 메모리로 다시 불러와야 한다. 비정상적으로 긴 일시 정지 현상이 잘 설명되었다.














